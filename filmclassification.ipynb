{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.datasets import imdb_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = imdb_dataset(train=True)  # doctest: +SKIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = imdb_dataset(test=True)  # doctest: +SKIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "    text=text.replace(\"\\t\", \"\")\n",
    "    all_reviews = text\n",
    "    all_words = text.split(\" \")\n",
    "    \n",
    "    return all_reviews, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_labels_separate(train):\n",
    "    labels=[]\n",
    "    text=[]\n",
    "    words=[]\n",
    "    for i in range(len(train)):\n",
    "        if train[i]['sentiment'] == 'pos':\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "        text.append(preprocess(train[i]['text'])[0])\n",
    "        words=words+preprocess(train[i]['text'])[1]\n",
    "    return text, labels, words\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, train_words= text_labels_separate(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts, test_labels, test_words= text_labels_separate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=train_words+test_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "word_list = sorted(word_counts, key=word_counts.get, reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clichélandmine',\n",
       " 'asymmetric',\n",
       " 'metaessay',\n",
       " 'warnabys',\n",
       " 'browfurrowingbr',\n",
       " 'secdonly',\n",
       " 'rena',\n",
       " 'bryces',\n",
       " 'wagonthey',\n",
       " '5m']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {word:idx+1 for idx, word in enumerate(word_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = {idx:word for word, idx in vocab_to_int.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_reviews_train = [[vocab_to_int[word] for word in review.split(' ')] for review in train_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_reviews_test = [[vocab_to_int[word] for word in review.split(' ')] for review in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for review in encoded_reviews_train + encoded_reviews_test:\n",
    "    length.append(len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 2470 mean: 231.14594 171.3247057534797\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('max:', max(length), 'mean:' , np.mean(length), np.std(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(encoded_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in encoded_reviews:\n",
    "        if len(review) <= seq_length:\n",
    "            reviews.append([0]*(seq_length-len(review)) + review )\n",
    "        else:\n",
    "            reviews.append(review[:seq_length])\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "\n",
    "padded_reviews_train = pad_text(encoded_reviews_train, seq_length = 250)\n",
    "padded_reviews_test = pad_text(encoded_reviews_test, seq_length = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0, 32611,   320,     6,     3,  1061,   214,     8,\n",
       "        2033,    30,     1,   163,    60,    14,    47,    78,  6088,\n",
       "          43,   376,   120,   137,    14,  4705,    54,  4658,   152,\n",
       "           7,     1,  4135,  5999,   463,    70,     5,   253,    11,\n",
       "       32611, 16737,  2055,     6,    73,  2480,     5,   616,    71,\n",
       "           6,  4705,     1, 29083,     5,  1929, 10398,     1,  5532,\n",
       "        1382,    37,    69,    65,   206,   141,    64,  1155,  4705,\n",
       "       24376,     1, 37717,     4,     1,   216,   864,    31,  3090,\n",
       "          70,     4,     1,  4574,     9,   653,     2,    64,  1382,\n",
       "          51,     9,   202,     1,   394,     7,    62,     3,  1421,\n",
       "        3275,   743,     5,  3542,   185,     1,   376,     9,  1182,\n",
       "       13444,    32,    30,    32,   320,     3,   353,   364,  3483,\n",
       "         142,   136,     5,  9483,    28,     4,   123,  4705,  1421,\n",
       "        2558,     5, 32611,   320,     9,   508,    11,   105,  1529,\n",
       "           4,    54,   640,   103,    11, 32611,   320,     6,   225,\n",
       "        7833,    49,     3,  2286,    11,     8,   203])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_reviews_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array( [label for idx, label in enumerate(train_labels) if len(padded_reviews_train[idx]) > 0] ).astype(float)\n",
    "reviews_train = [review for review in padded_reviews_train if len(review) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array( [label for idx, label in enumerate(test_labels) if len(padded_reviews_test[idx]) > 0] ).astype(float)\n",
    "reviews_test = [review for review in padded_reviews_test if len(review) > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test, test_labels = shuffle(reviews_test, test_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reviews_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.3\n",
    "total = len(reviews_test)\n",
    "valid_cutoff = int(total * valid_ratio)\n",
    "\n",
    "train_x = reviews_train\n",
    "train_y = train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=torch.Tensor(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250])\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=torch.Tensor(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x, valid_y = torch.Tensor(reviews_test[:valid_cutoff]), torch.Tensor(test_labels[:valid_cutoff])\n",
    "test_x, test_y = torch.Tensor(reviews_test[valid_cutoff:]), torch.Tensor(test_labels[valid_cutoff:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "valid_data = TensorDataset(valid_x, valid_y)\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ConvNet(nn.Module): \n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_hidden = n_hidden   # number of layers \n",
    "          \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.layer1 = nn.Sequential( nn.Conv1d(250, 248, kernel_size=2, stride=1), \n",
    "            nn.ReLU(), nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential( nn.Conv1d(248, 50, kernel_size=5, stride=4, padding=2), \n",
    "            nn.ReLU(), nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential( nn.Conv1d(50, 25, kernel_size=5, stride=4, padding=2), \n",
    "            nn.ReLU(), nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        self.soft=nn.Softmax()\n",
    "    \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        out = self.layer1(embedded_words)          # (batch_size, seq_length, n_hidden)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)  # (batch_size*seq_length, n_hidden)\n",
    "        out = self.dropout(out) \n",
    "        fc_out = self.fc(out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.soft(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        #sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab_to_int)\n",
    "n_embed = 100\n",
    "n_hidden = 300\n",
    "n_output = 2   # 1 (\"positive\") or 0 (\"negative\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "net = ConvNet(n_vocab, n_embed, n_hidden, n_output)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001 )\n",
    "# create a stochastic gradient descent optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EkStepanova\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Step: 200 Training Loss: -0.4800 Validation Loss: -0.4800\n",
      "Epoch: 1/10 Step: 400 Training Loss: -0.4400 Validation Loss: -0.4400\n",
      "Epoch: 2/10 Step: 600 Training Loss: -0.3800 Validation Loss: -0.3800\n",
      "Epoch: 2/10 Step: 800 Training Loss: -0.4000 Validation Loss: -0.4000\n",
      "Epoch: 2/10 Step: 1000 Training Loss: -0.7000 Validation Loss: -0.7000\n",
      "Epoch: 3/10 Step: 1200 Training Loss: -0.4800 Validation Loss: -0.4800\n",
      "Epoch: 3/10 Step: 1400 Training Loss: -0.4800 Validation Loss: -0.4800\n",
      "Epoch: 4/10 Step: 1600 Training Loss: -0.4201 Validation Loss: -0.4200\n",
      "Epoch: 4/10 Step: 1800 Training Loss: -0.4200 Validation Loss: -0.4200\n",
      "Epoch: 4/10 Step: 2000 Training Loss: -0.4401 Validation Loss: -0.4400\n",
      "Epoch: 5/10 Step: 2200 Training Loss: -0.5800 Validation Loss: -0.5800\n",
      "Epoch: 5/10 Step: 2400 Training Loss: -0.5600 Validation Loss: -0.5600\n",
      "Epoch: 6/10 Step: 2600 Training Loss: -0.5400 Validation Loss: -0.5400\n",
      "Epoch: 6/10 Step: 2800 Training Loss: -0.5000 Validation Loss: -0.5000\n",
      "Epoch: 6/10 Step: 3000 Training Loss: -0.5200 Validation Loss: -0.5200\n",
      "Epoch: 7/10 Step: 3200 Training Loss: -0.6400 Validation Loss: -0.6400\n",
      "Epoch: 7/10 Step: 3400 Training Loss: -0.5800 Validation Loss: -0.5800\n",
      "Epoch: 8/10 Step: 3600 Training Loss: -0.3400 Validation Loss: -0.3400\n",
      "Epoch: 8/10 Step: 3800 Training Loss: -0.5000 Validation Loss: -0.5000\n",
      "Epoch: 8/10 Step: 4000 Training Loss: -0.5400 Validation Loss: -0.5400\n",
      "Epoch: 9/10 Step: 4200 Training Loss: -0.6400 Validation Loss: -0.6400\n",
      "Epoch: 9/10 Step: 4400 Training Loss: -0.6000 Validation Loss: -0.6000\n",
      "Epoch: 10/10 Step: 4600 Training Loss: -0.5000 Validation Loss: -0.5000\n",
      "Epoch: 10/10 Step: 4800 Training Loss: -0.6400 Validation Loss: -0.6400\n",
      "Epoch: 10/10 Step: 5000 Training Loss: -0.5000 Validation Loss: -0.5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_every = 200\n",
    "step = 0\n",
    "n_epochs = 10  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device).long(), labels.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output= net(inputs)\n",
    "        loss = criterion(output, labels.long())\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device).long(), labels.to(device)\n",
    "        \n",
    "                \n",
    "                \n",
    "                v_output = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.long())\n",
    "                valid_losses.append(v_loss.item())\n",
    "                \n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EkStepanova\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test inputs: 50.011428571428574 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for test_in, labels in test_loader:\n",
    "        outputs = net(test_in.long())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test inputs: {} %'.format((correct / total) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EkStepanova\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4722\n",
      "Test Accuracy: 0.8350\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "acc_list = []\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    \n",
    "    test_output = net(inputs.long())\n",
    "    loss = criterion(test_output, labels.long())\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    #preds = torch.round(test_output)\n",
    "    #correct_tensor = preds.eq(labels.float())\n",
    "    #correct = np.squeeze(correct_tensor.numpy())\n",
    "    #num_correct += np.sum(correct)\n",
    "    #test_loss += criterion(test_output, labels.long()).data\n",
    "    #pred = test_output.data.max(1)[1] \n",
    "    _, predicted = torch.max(test_output.data, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    num_correct += np.sum(correct)\n",
    "    # get the index of the max log-probability\n",
    "    #num_correct += pred.eq(labels.data).sum()\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.4f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-50859c144794>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# Прямой запуск\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-158-12bbd901ae58>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_words)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                                              \u001b[1;31m# INPUT   :  (batch_size, seq_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0membedded_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_words\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# (batch_size, seq_length, n_embed)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_words\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# (batch_size, seq_length, n_hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m         return F.embedding(\n\u001b[0;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1722\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1724\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (reviews, labels) in enumerate(train_loader):\n",
    "        # Прямой запуск\n",
    "        outputs = model(reviews)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Обратное распространение и оптимизатор\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Отслеживание точности\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3150)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.NLLLoss()\n",
    "a = torch.tensor(([0.88, 0.12], [0.51, 0.49]), dtype = torch.float)\n",
    "target = torch.tensor([1, 0])\n",
    "output = loss(a, target)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9134)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losss=nn.CrossEntropyLoss()\n",
    "losss(a, target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
