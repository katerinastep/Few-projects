{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('canibalisation_set2.xlsx')[['str', 'mark']].drop_duplicates(keep='first').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39919 entries, 0 to 39918\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   index   39919 non-null  int64 \n",
      " 1   str     39919 non-null  object\n",
      " 2   mark    39919 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 935.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    #text = \",\".join([ch for ch in text])\n",
    "    #text=text.replace(\"\\t\", \"\")\n",
    "    all_reviews = text\n",
    "    all_words = text.split(',')\n",
    "    all_words = [word for word in all_words if word!='']\n",
    "    return all_reviews, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=[]\n",
    "for i in range(len(data)):\n",
    "    all_words=all_words+preprocess(data['str'][i])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " 'xада',\n",
       " '-',\n",
       " '-',\n",
       " '-']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3543 3351 19131\n"
     ]
    }
   ],
   "source": [
    "data0=data[data['mark']==0]\n",
    "data1=data[data['mark']==1]\n",
    "data2=data[data['mark']==2]\n",
    "print(len(data1), len(data2), len(data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full=data1.append(data0[:3351])\n",
    "data_full=data_full.append(data2[:3351])\n",
    "data_full=shuffle(data_full, random_state=42).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_labels_separate(train):\n",
    "    labels=[]\n",
    "    text=[]\n",
    "    for i in range(len(train)):\n",
    "        if train['mark'][i] == 0:\n",
    "            labels.append(0)#субсид\n",
    "            text.append(preprocess(train['str'][i])[0])\n",
    "        elif train['mark'][i] == 1:\n",
    "            labels.append(1)#\n",
    "            text.append(preprocess(train['str'][i])[0])\n",
    "        else: \n",
    "            labels.append(2)#\n",
    "            text.append(preprocess(train['str'][i])[0])\n",
    "       \n",
    "    return text, labels\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels= text_labels_separate(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "word_list = sorted(word_counts, key=word_counts.get, reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {word:idx+1 for idx, word in enumerate(word_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = {idx:word for word, idx in vocab_to_int.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-': 1,\n",
       " 'да': 2,\n",
       " 'др': 3,\n",
       " 'xа': 4,\n",
       " 'р': 5,\n",
       " 'а': 6,\n",
       " 'ада': 7,\n",
       " 'xада': 8,\n",
       " 'рдр': 9,\n",
       " 'дрда': 10,\n",
       " 'xадр': 11,\n",
       " 'xадрда': 12,\n",
       " 'адр': 13,\n",
       " 'адрда': 14,\n",
       " 'рда': 15,\n",
       " 'рдрда': 16}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-,-,да,ада,рдр,-,-,рдр,да,xада,да,-,-,да,-,да,-,-,да,рдр,-,др'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = [[vocab_to_int[word] for word in review.split(',') if word!=''] for review in train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 7, 9, 1, 1, 9, 2, 8, 2, 1, 1, 2, 1, 2, 1, 1, 2, 9, 1, 3]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 106 mean: 23.576476329917032 16.10757919263777\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for review in encoded_train:\n",
    "    length.append(len(review))\n",
    "\n",
    "print('max:', max(length), 'mean:' , np.mean(length), np.std(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(encoded_train, length):\n",
    "    \n",
    "    strings = []\n",
    "    \n",
    "    for string in encoded_train:\n",
    "        if len(string) <= length:\n",
    "            strings.append([0]*(length-len(string)) + string )\n",
    "        else:\n",
    "            strings.append(string[:length])\n",
    "        \n",
    "    return np.array(strings)\n",
    "\n",
    "\n",
    "padded_train = pad_text(encoded_train, length = 106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 7,\n",
       "       9, 1, 1, 9, 2, 8, 2, 1, 1, 2, 1, 2, 1, 1, 2, 9, 1, 3])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array( [label for idx, label in enumerate(train_labels) if len(padded_train[idx]) > 0] ).astype(float)\n",
    "strings_train = [string for string in padded_train if len(string) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.2\n",
    "test_ratio=0.2\n",
    "total = len(strings_train)\n",
    "train_cutoff = int(total * (1- (test_ratio+valid_ratio)))\n",
    "valid_cutoff = int(total* (1 - test_ratio))\n",
    "\n",
    "train_x, train_y = torch.Tensor(strings_train[:train_cutoff]), torch.Tensor(train_labels[:train_cutoff])\n",
    "valid_x, valid_y = torch.Tensor(strings_train[train_cutoff:valid_cutoff]), torch.Tensor(train_labels[train_cutoff:valid_cutoff])\n",
    "test_x, test_y = torch.Tensor(strings_train[valid_cutoff:]), torch.Tensor(train_labels[valid_cutoff:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "valid_data = TensorDataset(valid_x, valid_y)\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module): \n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_hidden = n_hidden   # number of layers \n",
    "          \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.layer1 = nn.Sequential( nn.Conv1d(106, 250, kernel_size=2, stride=1), \n",
    "            nn.ReLU(), nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential( nn.Conv1d(250, 100, kernel_size=5, stride=4, padding=2), \n",
    "            nn.ReLU(), nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        self.soft=nn.LogSoftmax()\n",
    "    \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        out = self.layer1(embedded_words)          # (batch_size, seq_length, n_kernel)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)  # (batch_size*seq_length, n_hidden)\n",
    "        out = self.dropout(out) \n",
    "        fc_out = self.fc(out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.soft(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        #sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        #sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab_to_int)+1\n",
    "n_embed = 50\n",
    "n_hidden = 300\n",
    "n_output = 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "net = CNN(n_vocab, n_embed, n_hidden, n_output)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EkStepanova\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 Step: 200 Training Loss: 0.3851 Validation Loss: 0.3449\n",
      "Epoch: 4/100 Step: 400 Training Loss: 0.3326 Validation Loss: 0.3190\n",
      "Epoch: 5/100 Step: 600 Training Loss: 0.1790 Validation Loss: 0.1509\n",
      "Epoch: 7/100 Step: 800 Training Loss: 0.3452 Validation Loss: 0.2726\n",
      "Epoch: 9/100 Step: 1000 Training Loss: 0.2941 Validation Loss: 0.2227\n",
      "Epoch: 10/100 Step: 1200 Training Loss: 0.2820 Validation Loss: 0.2049\n",
      "Epoch: 12/100 Step: 1400 Training Loss: 0.1584 Validation Loss: 0.0696\n",
      "Epoch: 14/100 Step: 1600 Training Loss: 0.1896 Validation Loss: 0.1036\n",
      "Epoch: 15/100 Step: 1800 Training Loss: 0.1637 Validation Loss: 0.0626\n",
      "Epoch: 17/100 Step: 2000 Training Loss: 0.1447 Validation Loss: 0.1183\n",
      "Epoch: 18/100 Step: 2200 Training Loss: 0.0811 Validation Loss: 0.0773\n",
      "Epoch: 20/100 Step: 2400 Training Loss: 0.0843 Validation Loss: 0.0461\n",
      "Epoch: 22/100 Step: 2600 Training Loss: 0.0792 Validation Loss: 0.0173\n",
      "Epoch: 23/100 Step: 2800 Training Loss: 0.0554 Validation Loss: 0.0291\n",
      "Epoch: 25/100 Step: 3000 Training Loss: 0.0467 Validation Loss: 0.0594\n",
      "Epoch: 27/100 Step: 3200 Training Loss: 0.0276 Validation Loss: 0.0143\n",
      "Epoch: 28/100 Step: 3400 Training Loss: 0.0518 Validation Loss: 0.0123\n",
      "Epoch: 30/100 Step: 3600 Training Loss: 0.1307 Validation Loss: 0.0413\n",
      "Epoch: 31/100 Step: 3800 Training Loss: 0.0828 Validation Loss: 0.0064\n",
      "Epoch: 33/100 Step: 4000 Training Loss: 0.0177 Validation Loss: 0.0091\n",
      "Epoch: 35/100 Step: 4200 Training Loss: 0.0150 Validation Loss: 0.0222\n",
      "Epoch: 36/100 Step: 4400 Training Loss: 0.0082 Validation Loss: 0.0190\n",
      "Epoch: 38/100 Step: 4600 Training Loss: 0.0076 Validation Loss: 0.0012\n",
      "Epoch: 40/100 Step: 4800 Training Loss: 0.0973 Validation Loss: 0.0269\n",
      "Epoch: 41/100 Step: 5000 Training Loss: 0.0407 Validation Loss: 0.0026\n",
      "Epoch: 43/100 Step: 5200 Training Loss: 0.0274 Validation Loss: 0.0255\n",
      "Epoch: 44/100 Step: 5400 Training Loss: 0.0228 Validation Loss: 0.0010\n",
      "Epoch: 46/100 Step: 5600 Training Loss: 0.0112 Validation Loss: 0.0085\n",
      "Epoch: 48/100 Step: 5800 Training Loss: 0.0482 Validation Loss: 0.0233\n",
      "Epoch: 49/100 Step: 6000 Training Loss: 0.0050 Validation Loss: 0.0014\n",
      "Epoch: 51/100 Step: 6200 Training Loss: 0.0075 Validation Loss: 0.0008\n",
      "Epoch: 53/100 Step: 6400 Training Loss: 0.0058 Validation Loss: 0.0018\n",
      "Epoch: 54/100 Step: 6600 Training Loss: 0.0113 Validation Loss: 0.0062\n",
      "Epoch: 56/100 Step: 6800 Training Loss: 0.0265 Validation Loss: 0.0076\n",
      "Epoch: 57/100 Step: 7000 Training Loss: 0.0781 Validation Loss: 0.0131\n",
      "Epoch: 59/100 Step: 7200 Training Loss: 0.0072 Validation Loss: 0.0002\n",
      "Epoch: 61/100 Step: 7400 Training Loss: 0.0176 Validation Loss: 0.0002\n",
      "Epoch: 62/100 Step: 7600 Training Loss: 0.0001 Validation Loss: 0.0001\n",
      "Epoch: 64/100 Step: 7800 Training Loss: 0.0016 Validation Loss: 0.0003\n",
      "Epoch: 66/100 Step: 8000 Training Loss: 0.0017 Validation Loss: 0.0002\n",
      "Epoch: 67/100 Step: 8200 Training Loss: 0.0014 Validation Loss: 0.0002\n",
      "Epoch: 69/100 Step: 8400 Training Loss: 0.0034 Validation Loss: 0.0004\n",
      "Epoch: 70/100 Step: 8600 Training Loss: 0.0003 Validation Loss: 0.0005\n",
      "Epoch: 72/100 Step: 8800 Training Loss: 0.1131 Validation Loss: 0.0024\n",
      "Epoch: 74/100 Step: 9000 Training Loss: 0.0019 Validation Loss: 0.0005\n",
      "Epoch: 75/100 Step: 9200 Training Loss: 0.0463 Validation Loss: 0.0020\n",
      "Epoch: 77/100 Step: 9400 Training Loss: 0.0057 Validation Loss: 0.0032\n",
      "Epoch: 79/100 Step: 9600 Training Loss: 0.0024 Validation Loss: 0.0002\n",
      "Epoch: 80/100 Step: 9800 Training Loss: 0.0548 Validation Loss: 0.0005\n",
      "Epoch: 82/100 Step: 10000 Training Loss: 0.0164 Validation Loss: 0.0300\n",
      "Epoch: 83/100 Step: 10200 Training Loss: 0.0003 Validation Loss: 0.0000\n",
      "Epoch: 85/100 Step: 10400 Training Loss: 0.0060 Validation Loss: 0.0001\n",
      "Epoch: 87/100 Step: 10600 Training Loss: 0.0038 Validation Loss: 0.0044\n",
      "Epoch: 88/100 Step: 10800 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 90/100 Step: 11000 Training Loss: 0.0074 Validation Loss: 0.0007\n",
      "Epoch: 92/100 Step: 11200 Training Loss: 0.0479 Validation Loss: 0.0001\n",
      "Epoch: 93/100 Step: 11400 Training Loss: 0.0675 Validation Loss: 0.0307\n",
      "Epoch: 95/100 Step: 11600 Training Loss: 0.0001 Validation Loss: 0.0001\n",
      "Epoch: 96/100 Step: 11800 Training Loss: 0.0084 Validation Loss: 0.0000\n",
      "Epoch: 98/100 Step: 12000 Training Loss: 0.0074 Validation Loss: 0.0001\n",
      "Epoch: 100/100 Step: 12200 Training Loss: 0.0106 Validation Loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "print_every = 200\n",
    "step = 0\n",
    "n_epochs = 100  # validation loss increases from ~ epoch 3 or 4\n",
    "#clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device).long(), labels.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output= net(inputs)\n",
    "        loss = criterion(output, labels.long())\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device).long(), labels.to(device)\n",
    "        \n",
    "                \n",
    "                \n",
    "                v_output = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.long())\n",
    "                valid_losses.append(v_loss.item())\n",
    "                \n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EkStepanova\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 2049 test inputs: 95.55880917520741 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for test_in, labels in test_loader:\n",
    "        outputs = net(test_in.long())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the', total, 'test inputs: {} %'.format((correct / total) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), 'canibalisation_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonmark=[]\n",
    "length = 100\n",
    "for i in range(len(data3)):\n",
    "    zero=preprocess(data3['str'][i])[0]\n",
    "    nonmark.append(zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_zero = [[vocab_to_int[word] for word in review.split(',') if word!=''] for review in nonmark]\n",
    "padded_zero = pad_text(encoded_zero, length = 100)\n",
    "zero_torch=torch.Tensor(padded_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_nonmark=[]\n",
    "net.eval()\n",
    "for i in range(len(data3)):\n",
    "    with torch.no_grad():\n",
    "        output = net(zero_torch[i].unsqueeze(0).long()).data.view(3).numpy()\n",
    "        res_nonmark.append(np.exp(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
